\subsection{Kronecker-factored Approximate Curvature}\label{sec:kfac-background}

We review Kronecker-factored approximate curvature (KFAC) which was introduced by~\citet{heskes2000natural, martens2015optimizing} in the context of maximum likelihood estimation to approximate the per-layer Fisher information matrix by a Kronecker product to speed up approximate natural gradient descent~\cite{amari1998natural}.
The Fisher associated with the loss $\nicefrac{1}{2N} \sum_{n=1}^N \left\lVert u_\vtheta(\vx_n) - y_n \right\rVert_2^2$ with targets $y_n \in \sR$ is
\begin{equation}\label{eq:fisher-mle}
  \mF(\vtheta)
  =
  \nicefrac{1}{N}
  \textstyle
  \sum_{n=1}^N
  \left(\jac_{\vtheta} u_{\vtheta}(\vx_n)  \right)^\top
  \jac_{\vtheta} u_{\vtheta}(\vx_n)
  =
  \nicefrac{1}{N}
  \textstyle
  \sum_{n=1}^N
  \left( \jac_{\vtheta} u_n \right)^\top
  \jac_{\vtheta} u_n
  \quad\in \sR^{P\times P}\,,
\end{equation}
where $u_n = u_\vtheta(\vx_n)$, and it coincides with the classical Gauss-Newton matrix~\citep{martens2020new}.
The established KFAC approximates \eqref{eq:fisher-mle}.
While the boundary Gramian $\mG_{\partial\Omega}(\vtheta)$ has the same structure as $\mF(\vtheta)$, the interior Gramian $\mG_\Omega(\vtheta)$ does not as it involves derivative rather than function evaluations of the net.

KFAC tackles the Fisher's per-layer block diagonal, $\mF(\vtheta) \approx \operatorname{diag}(\mF^{(1)}(\vtheta), \dots, \mF^{(L)}(\vtheta))$ with $\mF^{(l)}(\vtheta) = \nicefrac{1}{N} \sum_{n=1}^N (\jac_{\vtheta^{(l)}} u_n)^{\top} \jac_{\vtheta^{(l)}} u_n \in \sR^{ p^{(l)} \times p^{(l)} }$.
For a fully-connected layer's block, let's examine the term $\jac_{\vtheta^{(l)}} u_{\vtheta}(\vx)$ from \Cref{eq:fisher-mle} for a fixed data point.
The layer parameters $\vtheta^{(l)} = \flatten \mW^{(l)}$ enter the computation via $\vz^{(l)} = \mW^{(l)}\vz^{(l-1)}$ and we have $\jac_{\mW^{(l)}} \vz^{(l)} = {\vz^{(l-1)}}^\top \otimes \mI$ \citep[e.g.][]{dangel2020modular}.
Further, the chain rule gives the decomposition $\jac_{\mW^{(l)}} u = (\jac_{\vz^{(l)}} u) \jac_{\mW^{(l)}} \vz^{(l)} = {\vz^{(l-1)}}^\top\otimes \jac_{\vz^{(l)}} u$.
Inserting into $\mF^{(l)}(\vtheta)$, summing over data points, and using the expectation approximation $\sum_n \mA_n \otimes \mB_n \approx N^{-1}(\sum_n \mA_n) \otimes (\sum_n \mB_n)$ from \citet{martens2015optimizing}, we obtain the KFAC approximation for linear layers in supervised square loss regression with a network's output,
\begin{equation}\label{eq:kfac-linear}
  \mF^{(l)}(\vtheta)
  \approx
  \underbrace{
    \left(
      \nicefrac{1}{N}
      \textstyle
      \sum_{n=1}^N \vz^{(l-1)}_n {\vz^{(l-1)}_n}^\top
    \right)
  }_{\eqqcolon \mA^{(l)} \in \sR^{h^{(l-1)} \times h^{(l-1)}}}
  \otimes
  \underbrace{
    \left(
      \nicefrac{1}{N}
      \textstyle
      \sum_{n=1}^N
      \left(\jac_{\vz^{(l)}}  u_n \right)^\top
      \jac_{\vz^{(l)}}  u_n
    \right)
  }_{\eqqcolon \mB^{(l)} \in \sR^{h^{(l)} \times h^{(l)} }}\,.
\end{equation}
It is cheap to store and invert by inverting the two Kronecker factors.

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
