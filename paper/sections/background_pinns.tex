For simplicity, we present our approach for multi-layer perceptrons (MLPs) consisting of fully-connected and element-wise activation layers.
However, the generality of Taylor-mode automatic differentiation and KFAC for linear layers with weight sharing allow our KFAC to be applied to such layers (e.g.\,fully-connected, convolution, attention) in arbitrary neural network architectures.

\textbf{Flattening \& Derivatives}
We vectorize matrices using the \emph{first-index-varies-fastest} convention, i.e.\,column-stacking (row index varies first, column index varies second) and denote the corresponding flattening operation by $\flatten$.
This allows to reduce derivatives of matrix- or tensor-valued objects back to the vector case by flattening a function's input and output before differentiation.
The Jacobian of a vector-to-vector function $\va \mapsto \vb(\va)$ has entries $[\jac_{\va}\vb]_{i,j} = \nicefrac{\partial \evb_i}{\partial \eva_j}$.
For a matrix-to-matrix function $\mA \mapsto \mB(\mA)$, the Jacobian is $\jac_{\mA} \mB = \jac_{\flatten \mA }\flatten\mB$.
A useful property of $\flatten$ is $\flatten(\mA\mX\mB) = (\mB^\top\otimes \mA)\flatten{\mX}$ for matrices $\mA, \mX, \mB$ which implies $\jac_\mX(\mA\mX\mB) = \mB^\top\otimes \mA$.

\textbf{Sequential neural nets} Consider a \emph{sequential neural network} $u_{\vtheta} = f_{\vtheta^{(L)}} \circ f_{\vtheta^{(L-1)}} \circ \ldots \circ f_{\vtheta^{(1)}} $ of depth $L\in\mathbb N$. It consists of layers $f_{\vtheta^{(l)}}\colon \sR^{h^{(l-1)}}\to\sR^{h^{(l)}}$, $\vz^{(l-1)}\mapsto \vz^{(l)} = f_{\vtheta^{(l)}}(\vz^{(l-1)})$ with trainable parameters $\vtheta^{(l)} \in \sR^{p^{(l)}}$ that transform an input $\vz^{(0)} \coloneqq \vx \in \mathbb R^{d \coloneqq h^{(0)}}$ into a prediction $u_\vtheta(\vx) = \vz^{(L)} \in \sR^{h^{(L)}}$ via intermediate representations $\vz^{(l)} \in \sR^{h^{(l)}}$.
In the context of PINNs, we use networks with scalar outputs ($h^{(L)}=1$) and denote the concatenation of all parameters by $\vtheta = (\vtheta^{(1)\top}, \dots, \vtheta^{(L)\top})^{\top} \in \sR^P$.
A common choice is to alternate fully-connected and activation layers.
Linear layers map $\vz^{(l-1)} \mapsto \vz^{(l)} = \mW^{(l)} \vz^{(l-1)}$ using a weight matrix $\mW^{(l)} = \flatten^{-1}\vtheta^{(l)}  \in \sR^{h^{(l)} \times h^{(l-1)}}$ (bias terms can be added as an additional column and by appending a $1$ to the input).
Activation layers map $\vz^{(l-1)}\mapsto \vz^{(l)} = \sigma(\vz^{(l-1)})$ element-wise for a (typically smooth) $\sigma\colon\mathbb R\to\mathbb R$.

\subsection{Energy Natural Gradients for Physics-Informed Neural Networks}
Let us consider a domain $\Omega\subseteq\mathbb R^d$ and the partial differential equation
\begin{align*}
  \mathcal{L} u = f \quad \text{in }\Omega\,,
                   \qquad
  u = g \quad \text{on }\partial\Omega\,,
\end{align*}
with right-hand side $f$, boundary data $g$ and a differential operator
$\mathcal{L}$, e.g.\,the negative Laplacian $-\mathcal{L} u = \Delta_{\vx} u = \sum_{i=1}^d \partial_{\evx_i}^2 u$.
We parametrize $u$ using a neural network and train its parameters $\vtheta$ to minimize the loss
\begin{align}\label{eq:pinn-loss}
  \!\!\!\!\!\!
  L(\vtheta)
  &=
    \frac{1}{2N_\Omega} \sum_{n=1}^{N_\Omega} (\mathcal{L} u_\vtheta(\vx_n) - f(\vx_n))^2
    +
    \frac{1}{2N_{\partial\Omega}}\sum_{n=1}^{N_{\partial\Omega}} ( u_\vtheta(\vx^\text{b}_n) - g(\vx^\text{b}_n))^2
    \eqqcolon
    L_\Omega(\vtheta) + L_{\partial\Omega}(\vtheta)
\end{align}
with points $\{\vx_n \in \Omega \}_{n=1}^{N_\Omega}$ from the domain's interior, and points $\{\vx^\text{b}_n \in \partial\Omega \}_{n=1}^{N_{\partial\Omega}}$ on its boundary.\footnote{The second regression loss can also include other constraints like measurement data.}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
