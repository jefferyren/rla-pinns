This subsection describes the Kronecker approximation we propose for the per-layer Gramian and justify that the approximation is similar to the concepts used in other KFAC works.
We will rely on a similar intuition than \citet{eschenhagen2023kroneckerfactored} in the presence of weight sharing.
However, we will use a slightly different motivation to obtain the Kronecker structure in the presence of weight sharing.

\begin{table}
  \centering
  \caption{Overview of Kronecker approximations for matrix multiplication with a weight $\mW \in \sR^{D_1 \times D_2}$ in the presence and absence of weight sharing and transposition.
    Transposition changes to which Kronecker factor the input/grad output contributes.
    Weight sharing requires an additional approximation to obtain a single Kronecker factor.
  }\label{tab:kronecker-approximations-basic-operations}
  \resizebox{\linewidth}{!}{%
    \begin{tabular}{cccccc}
      \toprule
      (share,
      &
        Input
      &
        Output
      &
        Grad output
      &
        Jacobian
      &
        Gramian proxy
      \\
      \quad transp.)
      &
        $\tX$
      &
        $\tZ$
      &
        $\grad{\tZ}\ell$ % \coloneqq \grad{\flatten(\tZ)}\ell$
      &
        $\jac_{\mW}\tZ$ % \coloneqq \jac_{\flatten(\mW)}\flatten(\tZ)$
      &
        $(\jac_{\mW}\tZ)^{\top} \grad{\tZ}\ell (\grad{\tZ}\ell)^{\top} \jac_{\mW}\tZ
        \approx \mA \otimes \mB$
      \\
      \midrule
      (\xmark, \xmark)
      &
        $\vx \in \sR^{D_2}$
      &
        $\vz = \mW \vx \in \sR^{D_1}$
      &
        $\grad{\vz}\ell \in \sR^{D_1}$
      &
        $\vx^{\top} \otimes \mI_{D_1}$
      &
        $\vx \vx^{\top} \otimes \grad{\vz}\ell (\grad{\vz}\ell)^{\top}$
      \\
      (\xmark, \cmark)
      &
        $\vx \in \sR^{D_1}$
      &
        $\vz = \mW^{\top} \vx \in \sR^{D_2}$
      &
        $\grad{\vz}\ell \in \sR^{D_2}$
      &
        $\mI_{D_2} \otimes \vx^{\top}$
      &
        $\grad{\vz}\ell (\grad{\vz}\ell)^{\top} \otimes \vx \vx^{\top}$
      \\
      (\cmark, \xmark)
      &
        $\mX \in \sR^{D_2 \times S}$
      &
        $\mZ = \mW \mX \in \sR^{D_1 \times S}$
      &
        $\grad{\mZ}\ell \in \sR^{S D_1}$
      &
        $\mX^{\top} \otimes \mI_{D_1}$
      &
        $
        \mX \mX^{\top}
        \otimes
        \frac{1}{S}
        \flatten^{-1}(\grad{\mZ}\ell) {\flatten^{-1}(\grad{\mZ}\ell) }^{\top}
        $
      \\
      (\cmark, \cmark)
      &
        $\mX \in \sR^{D_1 \times S}$
      &
        $\mZ = \mW^{\top} \mX \in \sR^{D_2 \times S}$
      &
        $\grad{\mZ}\ell \in \sR^{S D_2}$
      &
        $\mK \left( \mI_{D_2} \otimes \mX^{\top} \right)$
      &
        $
        \frac{1}{S}
        \flatten^{-1}(\grad{\mZ}\ell) {\flatten^{-1}(\grad{\mZ}\ell)}^{\top}
        \otimes
        \mX \mX^{\top}
        $
      \\
      New
      &
        $\mX \in \mathrm{Sym}(D_1)$
      &
        $\mZ = \mW^{\top} \mX \mW \in \mathrm{Sym}(D_2)$
      &
        $\grad{\mZ}\ell \in \sR^{D_2^2}$
      &
        $\mI \otimes \mW^{\top} \mX + \mK \left( \mI \otimes \mW^{\top} \mX^{\top} \right)$
        &
          $\frac{4}{D_2} \flatten^{-1}(\grad{\mZ}\ell) {\flatten^{-1}(\grad{\mZ}\ell)}^{\top} \otimes \mX \mW \mW^{\top} \mX$
      \\
      \bottomrule
    \end{tabular}
  }
\end{table}


\paragraph{Some intuition:} Consider a weight matrix $\mW \in \sR^{D_1 \times
  D_2}$ inside a neural network that produces a loss $\ell$. We want to compute
the Gramian $\mF(\mW) = \grad{\mW}\ell (\grad{\mW}\ell)^{\top}$. Consider the following scenarios:
\begin{enumerate}
\item \textbf{(No sharing, no transpose)} The matrix is used to form the matrix vector product $\vz = \mW \vx \in \sR^{D_2}$ with an input vector $\vx \in \sR^{D_1}$.
  The Gramian is then
  \begin{align*}
    \mF(\mW) = \vx \vx^{\top} \otimes \grad{\vz}\ell (\grad{\vz}\ell)^{\top}\,.
  \end{align*}
  The input to the matrix multiply forms the first Kronecker factor, the gradient w.r.t.\,the matrix multiply's output forms the second Kronecker factor.

\item \textbf{(No sharing, transpose)} The matrix is used to form the transpose matrix vector product $\vz = \mW^{\top} \vx \in \sR^{D_1}$.
  The Gramian is then
  \begin{align*}
    \mF(\mW) = \grad{\vz}\ell (\grad{\vz}\ell)^{\top} \otimes \vx \vx^{\top}\,.
  \end{align*}
  The gradient w.r.t.\,the transpose matrix multiply's output forms the first Kronecker factor, the input to the matrix multiply forms the second Kronecker factor.
  I.e.
  the Gramian w.r.t.\,the transpose matrix is just the Gramian of the non-transposed matrix, but with swapped Kronecker factors.

\item \textbf{(Sharing, no transpose)} The matrix is used to form the matrix matrix product $\mZ = \mW \mX \in \sR^{D_2 \times S}$ with an input matrix $\mX \in \sR^{D_1 \times S}$ consisting of $S$ vector-valued columns which share the same weights.
  The Gramian is then
  \begin{align*}
    \mF(\mW) =
    \left(
    \mX
    \otimes
    \mI_{D_2}
    \right)
    \grad{\mZ}\ell (\grad{\mZ}\ell)^{\top}
    \left(
    \mX^{\top}
    \otimes
    \mI_{D_2}
    \right)
  \end{align*}
  Note that this does not simplify into a single Kronecker product.
  We need to apply another approximation.
  One way to obtain a single Kronecker factor is to seek an approximation for the central term such that $\grad{\mZ}\ell (\grad{\mZ}\ell)^{\top} \approx \mI \otimes \mU$ with $\mU \in \sR^{D_1 \times D_1}$.
  Let's look for the matrix that minimizes the reconstruction error
  $\left\lVert \grad{\mZ}\ell (\grad{\mZ}\ell)^{\top} - \mI \otimes \mU
  \right\rVert_{\text{F}}^2$. The solution to this is given by the average
  diagonal blocks of $\grad{\mZ}\ell (\grad{\mZ}\ell)^{\top}$. We can write it
  as
  \begin{align}
    \mU
    =
    \frac{1}{S}
    \flatten^{-1}
    \left(
    \grad{\mZ}\ell
    \right)
    \left[
    \flatten^{-1}
    \left(
    \grad{\mZ}\ell
    \right)
    \right]^{\top}
  \end{align}
  where $\flatten^{-1} \left(\grad{\mZ}\ell \right) \in \sR^{D_1 \times S}$.
  With this approximation, we can express the Gramian as a single Kronecker product:
  \begin{align*}
    \begin{split}
      \mF(\mW)
      &\approx
        \left(
        \mX
        \otimes
        \mI_{D_2}
        \right)
        \left(
        \mI
        \otimes
        \mU
        \right)
        \left(
        \mX^{\top}
        \otimes
        \mI_{D_2}
        \right)
      \\
      &=
        \mX \mX^{\top}
        \otimes
        \mU
      \\
      &=
        \mX \mX^{\top}
        \otimes
        \frac{1}{S}
        \flatten^{-1}
        \left(
        \grad{\mZ}\ell
        \right)
        \left[
        \flatten^{-1}
        \left(
        \grad{\mZ}\ell
        \right)
        \right]^{\top}
    \end{split}
  \end{align*}
  Again, note that the input to the matrix multiply forms the first Kronecker factor, the gradient w.r.t.\,the matrix multiply's output forms the second Kronecker factor.
  However, we needed additional approximations to obtain a single Kronecker product.

\item \textbf{(Sharing, transpose)} The matrix is used to form the transpose matrix matrix product $\mZ = \mW^{\top} \mX \in \sR^{D_1 \times S}$ with $\mX \in \sR^{D_2 \times S}$.
  The Gramian is then
  \begin{align*}
    \begin{split}
      \mF(\mW)
      &=
        \left(
        \mI_{D_1}
        \otimes
        \mX
        \right)
        \mK^{\top}
        \grad{\mZ}\ell (\grad{\mZ}\ell)^{\top}
        \mK
        \left(
        \mI_{D_1}
        \otimes
        \mX^{\top}
        \right)\,.
    \end{split}
  \end{align*}
  Again, this does not simplify into a single Kronecker product.
  So we are forced to make another approximation.
  Let's first take a closer look at $\mK^{\top} \grad{\mZ}\ell$.
  The application of $\mK^{\top}$ simply changes the flattening scheme of the vector.
  With the definition $\grad{\mZ^{\top}}\ell := \grad{\flatten(\mZ^{\top})} \ell$, we have that $\mK^{\top} \grad{\mZ}\ell = \grad{\mZ^{\top}}\ell$.
  Just like in the (sharing, no transpose) case from above, we will now look for an approximation of $\grad{\mZ^{\top}}\ell (\grad{\mZ^{\top}}\ell)^{\top} \approx \mU \otimes \mI$ with $\mU \in \sR^{D_1\times D_1}$.
  Again, we choose $\mU$ to minimize the reconstruction $\left\lVert \grad{\mZ^{\top}}\ell (\grad{\mZ^{\top}}\ell)^{\top} - \mU \otimes \mI \right\rVert_{\text{F}}^2$.
  We can transform this by applying $\mK$ from the left and from the right before evaluating the squared Frobenius norm (this only rearranges the elements and the Frobenius norm does not depend on the order).
  So we have $\left\lVert \mK \left( \grad{\mZ^{\top}}\ell (\grad{\mZ^{\top}}\ell)^{\top} - \mU \otimes \mI \right) \mK \right\rVert_{\text{F}}^2 = \left\lVert \grad{\mZ}\ell (\grad{\mZ}\ell)^{\top} - \mI \otimes \mU \right\rVert_{\text{F}}^2 $, which is the same objective from the (sharing, no transpose) case.
  Hence, its solution is $\mU = \nicefrac{1}{S} \flatten^{-1}
  \left(\grad{\mZ}\ell \right) \left[\flatten^{-1} \left(\grad{\mZ}\ell \right)
  \right]^{\top}$. With that, the single Kronecker product approximation of the
  Fisher is
  \begin{align*}
    \begin{split}
      \mF^{(i)}
      &\approx
        \left(
        \mI_{D_1}
        \otimes
        \mX
        \right)
        \left(
        \mU
        \otimes
        \mI_{S}
        \right)
        \left(
        \mI_{D_1}
        \otimes
        \mX^{\top}
        \right)
      \\
      &=
        \mU
        \otimes
        \mX \mX^{\top}
      \\
      &=
        \frac{1}{S}
        \flatten^{-1}
        \left(
        \grad{\mZ}\ell
        \right)
        \left[
        \flatten^{-1}
        \left(
        \grad{\mZ}\ell
        \right)
        \right]^{\top}
        \otimes
        \mX \mX^{\top}
    \end{split}
  \end{align*}
  The input to the transpose matrix multiply forms the first Kronecker factor, the gradient w.r.t.\,the transpose matrix multiply's output forms the second Kronecker factor.

\end{enumerate}

\paragraph{Single datum case} As a first step, let's write down only the diagonal terms of the Gramian, i.e.\,the terms caused by the same children and try to simplify each term into a Kronecker product of same dimension:
\begin{align}
  \begin{split}
    \mF^{(i)}
    &\approx
      \underbrace{
      \left(
      {\vz^{(i-1)}}^\top\otimes \mI
      \right)^{\top}
      \grad{\vz^{(i)}}\Delta u
      \left(
      \left(
      {\vz^{(i-1)}}^\top\otimes \mI
      \right)^{\top}
      \grad{\vz^{(i)}}\Delta u
      \right)^{\top}
      }_{(1, 1)}
    \\
    &\phantom{=}+
      \underbrace{
      \left(
      \mI \otimes \grad{\vz^{(i)}}u
      \right)^{\top}
      \grad{\grad{\vz^{(i-1)}}u}\Delta u
      \left(
      \left(
      \mI \otimes \grad{\vz^{(i)}}u
      \right)^{\top}
      \grad{\grad{\vz^{(i-1)}}u}\Delta u
      \right)^{\top}
      }_{(2, 2)}
    \\
    &\phantom{=}+
      \underbrace{
      2
      \left(
      \mI \otimes
      \left[
      \left( \gradsquared{\vz^{(i)}}u \right) \mW^{(i)}
      \right]
      \right)
      \grad{\gradsquared{\vz^{(i-1)}}u}\Delta u
      \left(
      2
      \left(
      \mI \otimes
      \left[
      \left( \gradsquared{\vz^{(i)}}u \right) \mW^{(i)}
      \right]
      \right)
      \grad{\gradsquared{\vz^{(i-1)}}u}\Delta u
      \right)^{\top}
      }_{(3, 3)}
      \,.
      \shortintertext{Without approximations, we can re-write this as}
    &=
      \vz^{(i-1)} {\vz^{(i-1)}}^\top
      \otimes
      \left(
      \grad{\vz^{(i)}}\Delta u
      \right)
      \left(
      \grad{\vz^{(i)}}\Delta u
      \right)^{\top}
    \\
    &\phantom{=}+
      \left(
      \grad{\vz^{(i)}}u
      \right)
      \left(
      \grad{\vz^{(i)}}u
      \right)^{\top}
      \otimes
      \left(
      \grad{\grad{\vz^{(i-1)}}u}\Delta u
      \right)
      \left(
      \grad{\grad{\vz^{(i-1)}}u}\Delta u
      \right)^{\top}
    \\
    &\phantom{=}+
      4
      \left(
      \mI \otimes
      \left[
      \left( \gradsquared{\vz^{(i)}}u \right) \mW^{(i)}
      \right]
      \right)
      \left[
      \left(
      \grad{\gradsquared{\vz^{(i-1)}}u}\Delta u
      \right)
      \left(
      \grad{\gradsquared{\vz^{(i-1)}}u}\Delta u
      \right)^{\top}
      \right]
      \left(
      \mI \otimes
      \left[
      {\mW^{(i)}}^{\top}
      \left( \gradsquared{\vz^{(i)}}u \right)
      \right]
      \right)
      \,.
      \intertext{The first two terms have the same Kronecker structure, $h^{(i-1)} \times h^{(i-1)} \otimes h^{(i)} \times h^{(i)}$.
      The third term does not simplify further, because the non-identity term in the Jacobians (outer terms) is a matrix, not a vector.
      One way to obtain the same Kronecker structure is to approximate the central term $ \left(\grad{\gradsquared{\vz^{(i-1)}}u}\Delta u \right) \left(\grad{\gradsquared{\vz^{(i-1)}}u}\Delta u \right)^{\top} \approx \mU \otimes \mI$ where $\mU \in \sR^{h^{(i-1)}\times h^{(i-1)}}$ (remember that $\grad{\gradsquared{\vz^{(i-1)}}}u \in \sR^{{h^{(i-1)}}^2}$.
      In the following, let $g := \grad{\gradsquared{\vz^{(i-1)}}}u$ for brevity.
      The `optimal' $\mU$ that minimizes the Frobenius norm $\left\lVert\mU \otimes \mI - \vg \vg^{\top} \right\rVert_{\text{F}}^2$ is the averaged block diagonal $\mU = \nicefrac{1}{h^{(i-1)}} \mG \mG^{\top}$ where $\mG = \flatten^{-1}(\vg) \in \sR^{h^{(i-1)} \times h^{(i-1)}}$.
      With this additional approximation, we can also express the third term as a Kronecker product:}
    &\approx
      \vz^{(i-1)} {\vz^{(i-1)}}^\top
      \otimes
      \left(
      \grad{\vz^{(i)}}\Delta u
      \right)
      \left(
      \grad{\vz^{(i)}}\Delta u
      \right)^{\top}
    \\
    &\phantom{=}+
      \left(
      \grad{\vz^{(i)}}u
      \right)
      \left(
      \grad{\vz^{(i)}}u
      \right)^{\top}
      \otimes
      \left(
      \grad{\grad{\vz^{(i-1)}}u}\Delta u
      \right)
      \left(
      \grad{\grad{\vz^{(i-1)}}u}\Delta u
      \right)^{\top}
    \\
    &\phantom{=}+
      \frac{4}{h^{(i-1)}}
      \left(
      \flatten^{-1}
      \left(
      \grad{\gradsquared{\vz^{(i-1)}}u}\Delta u
      \right)
      \right)
      \left(
      \flatten^{-1}
      \left(
      \grad{\gradsquared{\vz^{(i-1)}}u}\Delta u
      \right)
      \right)^{\top}
      \otimes
      \left( \gradsquared{\vz^{(i)}}u \right)
      \mW^{(i)}
      {\mW^{(i)}}^{\top}
      \left( \gradsquared{\vz^{(i)}}u \right)
      \,.
      \shortintertext{Let's make the final approximation from three Kronecker products into a single Kronecker product.
      In KFAC-style, we use a Kronecker-of-sums to approximate a sum-of-Kroneckers:}
    &\approx
      \mA^{(i)} \otimes \mB^{(i)}
  \end{split}
\end{align}
with
\begin{subequations}
  \label{eq:gram-kronecker-approximations-unbatched}
  \begin{align}
    \begin{split}
      \mA^{(i)}
      &=
        \vz^{(i-1)} {\vz^{(i-1)}}^\top
        +
        \left(
        \grad{\vz^{(i)}}u
        \right)
        \left(
        \grad{\vz^{(i)}}u
        \right)^{\top}
      \\
      &\phantom{=}+
        \frac{4}{h^{(i-1)}}
        \left(
        \flatten^{-1}
        \left(
        \grad{\gradsquared{\vz^{(i-1)}}u}\Delta u
        \right)
        \right)
        \left(
        \flatten^{-1}
        \left(
        \grad{\gradsquared{\vz^{(i-1)}}u}\Delta u
        \right)
        \right)^{\top}\,,
    \end{split}
    \\
    \begin{split}
      \mB^{(i)}
      &=
        \left(
        \grad{\vz^{(i)}}\Delta u
        \right)
        \left(
        \grad{\vz^{(i)}}\Delta u
        \right)^{\top}
        +
        \left(
        \grad{\grad{\vz^{(i-1)}}u}\Delta u
        \right)
        \left(
        \grad{\grad{\vz^{(i-1)}}u}\Delta u
        \right)^{\top}
      \\
      &\phantom{=}+
        \left( \gradsquared{\vz^{(i)}}u \right)
        \mW^{(i)}
        {\mW^{(i)}}^{\top}
        \left( \gradsquared{\vz^{(i)}}u \right)\,.
    \end{split}
  \end{align}
\end{subequations}

\paragraph{With batching} In the presence of multiple data points, we need to approximate the Gramian $\mF^{(i)} \approx \nicefrac{1}{N} \sum_{n=1}^N \mA_n^{(i)} \otimes \mB_n^{(i)}$ further (the subscripts $_n$ denote the computation on datum $n$).
We can just do that in the same way as the original KFAC paper, which gives us
\begin{align}\label{eq:gram-kronecker-approximations-batched}
  \mF^{(i)}
  \approx
  \left(
  \frac{1}{N}\sum_{n=1}^N \mA_n^{(i)}
  \right)
  \otimes
  \left(
  \sum_{n=1}^N \mB_n^{(i)}
  \right)
\end{align}
\Cref{eq:gram-kronecker-approximations-unbatched,eq:gram-kronecker-approximations-batched} are our proposed approximation for the Gramian.

\section{Different Gramians}

\subsection{$L^2$ aka Gauss-Newton matrix aka empirical Fisher}

In least squares regression with data $(\vx_1, \vy_1), \dots, (\vx_N, \vy_N)$ the Gramian $\mG(\vtheta)$ is chosen as the (empirical) Fisher information matrix given in~\citep{martens2020new, eschenhagen2023kroneckerfactored}
\begin{equation}
  \mG(\vtheta) = \frac1N\sum_{n=1}^N \jac_{\vtheta} u_{\vtheta}(\vx_n)^\top \jac_{\vtheta} u_{\vtheta}(\vx_n).
 \end{equation}
 The individual entries are given by
\begin{equation}
  \mG(\vtheta)_{ij} = \frac1N\sum_{n=1}^N \partial_{\vtheta_i} u_{\vtheta}(\vx_n) \partial_{\vtheta_j} u_{\vtheta}(\vx_n).
\end{equation}
Another way to derive this preconditioner is from the standpoint of maximum likelihood estimation.
Indeed, let us consider the following parametric distribution $p_\theta(\cdot|\vx) = \mathcal N(u_\vtheta(\vx), \sigma^2\mI)$, i.e., the density of the conditional distribution is given by
\begin{align}
    p_\vtheta(\vy|\vx) = \frac1{(2\pi\sigma^2)^{\frac{d}2}} \cdot e^{-\frac{\lVert u_\vtheta(\vx) - \vy \rVert_2^2}{2\sigma^2}}.
\end{align}
I.e., we interpret the output of the network as an estimate of the mean and then assume a standard Gaussian noise.
The \emph{Fisher information matrix} of the probability model
\begin{align}
    p_\vtheta = \frac1N \sum_{n=1}^N \delta_{\vx_n}\otimes p_\vtheta(\cdot|\vx_n)
\end{align}
\todo[inline]{I just recalled the confusion that about the what people call the empirical Fisher; what I meant was the empirical Fisher as referred to by Amari, what I wrote down was the empirical Fisher in the ML community, which is something different; nicely explained of course by \cite{kunstner2019limitations}}
on $\mathbb X\times\mathbb Y$ is given by
\begin{align}
    {\mF}(\vtheta)_{ij} = \frac1N\sum_{n=1}^N  \mathbb E_{p_\vtheta(\vy|\vx_n)}\Big[\partial_{\vtheta_i} \log p_{\vtheta}(\vy|\vx_n) \partial_{\vtheta_j} \log p_{\vtheta}(\vy|\vx_n)\Big].
\end{align}
A short computation shows that up to an absolute constant factor $\mG$ and ${\mF}$ agree.
Another way of stating this is that $u\mapsto p$ is an isometry wrt to $L^2$ and the Fisher-Rao metric...
\todo{}

One can also consider the empirical empirical Fisher
\begin{align}
    \hat{\mF}(\vtheta)_{ij} = \frac1N\sum_{n=1}^N  \partial_{\vtheta_i} \log p_{\vtheta}(\vy_n|\vx_n) \partial_{\vtheta_j} \log p_{\vtheta}(\vy_n|\vx_n).
\end{align}

\subsection{Quantum Gramian / Quantum Natural Gradient}

The \emph{Quantum natural gradient} was introduced in~\cite{stokes2020quantum} in the context of variational Monte-Carlo methods. Here, one parametrizes wave functions $\psi_\vtheta$ of some Quantum system  %$p_\vtheta\propto \psi_\vtheta^2$, where $\psi_\vtheta$ is computed by some neural network, i.e.,
and consequently considers the corresponding probability density functions
\begin{align}
    p_\vtheta(\vx) = \frac{\psi_\vtheta^2(\vx)}{\lVert \psi_\vtheta \rVert_2^2}.
\end{align}
We compute
\begin{align}
    \partial_{\vtheta_i} \log p_{\vtheta}(\vx) & = \partial_{\vtheta_i} \log(\psi_{\vtheta}(\vx)^2) - \partial_{\vtheta_i} \log(\lVert \psi_{\vtheta}\rVert_2^2)
    \\ & = \frac{\partial_{\vtheta_i} \psi_\vtheta(\vx)^2}{\psi_\vtheta(\vx)^2} - \frac{\partial_{\vtheta_i} \lVert\psi_\vtheta \rVert_2^2}{\lVert\psi_\vtheta \rVert_2^2}
    \\ & = \frac{2\psi_\vtheta(\vx)\partial_{\vtheta_i} \psi_\vtheta(\vx)}{\psi_\vtheta(\vx)^2} - \frac{2 \langle \psi_\vtheta, \partial_{\vtheta_i} \psi_\vtheta \rangle }{\lVert\psi_\vtheta \rVert_2^2}
\end{align}
The corresponding Fisher-information matrix is given by
\begin{align*}
    \frac{\mF_Q(\vtheta)_{ij}}{4} & = \frac{\mathbb E_{p_\vtheta}[\partial_{\vtheta_i} \log p_{\vtheta}(\vx)\partial_{\vtheta_j} \log p_{\vtheta}(\vx)]}{4}
    \\ & = \mathbb E_{p_\vtheta}\left[ \frac{\partial_{\vtheta_i} \psi_\vtheta \partial_{\vtheta_j} \psi_\vtheta}{\psi_\vtheta^2} \right] - \frac{\langle \psi_{\vtheta}, \partial_{\vtheta_i} \psi_\vtheta \rangle}{ \lVert \psi_\vtheta \rVert_2^2 } \cdot \mathbb E_{p_\vtheta} \left[ \frac{\partial_{\vtheta_j} \psi_\vtheta}{\psi_\vtheta} \right] - \frac{\langle \psi_{\vtheta}, \partial_{\vtheta_j} \psi_\vtheta \rangle}{ \lVert \psi_\vtheta \rVert_2^2 } \cdot \mathbb E_{p_\vtheta} \left[ \frac{\partial_{\vtheta_i} \psi_\vtheta}{\psi_\vtheta} \right]
    \\ & \qquad + \frac{\langle \psi_{\vtheta}, \partial_{\vtheta_i} \psi_\vtheta \rangle\langle \psi_{\vtheta}, \partial_{\vtheta_j} \psi_\vtheta \rangle}{\lVert \psi_\vtheta \rVert_2^4}
    \\ & = \frac{\langle \partial_{\vtheta_i} \psi_{\vtheta}, \partial_{\vtheta_j} \psi_\vtheta \rangle}{\lVert \psi_\vtheta \rVert_2^2} - \frac{\langle \psi_{\vtheta}, \partial_{\vtheta_i} \psi_\vtheta \rangle\langle \psi_{\vtheta}, \partial_{\vtheta_j} \psi_\vtheta \rangle}{\lVert \psi_\vtheta \rVert_2^4}.
\end{align*}
Note that if $\lVert \psi_\vtheta \rVert_2^2=1$ for all $\vtheta$, then
\begin{align}
    0 = \partial_{\vtheta_i} \lVert \psi_\vtheta \rVert_2^2 = 2\langle \psi_{\vtheta}, \partial_{\vtheta_i} \psi_\vtheta \rangle
\end{align}
and hence the Gramian simplifies to
\begin{align}
\mF_Q(\vtheta)_{ij} = 4\langle \partial_{\vtheta_i} \psi_{\vtheta}, \partial_{\vtheta_j} \psi_\vtheta \rangle.
\end{align}
However, it is not clear to me how this can be enforced for a neural network architecture.

\subsection{Rayleigh-Gauss-Newton}
This was introduced in~\cite{webber2022rayleigh} and leads to a Gramian that incorporates PDE / Hamiltonian terms.
An important task in computational quantum mechanics is to minimize the Rayleigh quotient
\begin{align}
    \mathcal E(\psi) = \frac{\langle \psi, \mathcal H \psi \rangle}{\langle \psi, \psi \rangle},
\end{align}
where $\mathcal H$ denotes the \emph{Hamiltonian} of the quantum system.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../main"
%%% End:
